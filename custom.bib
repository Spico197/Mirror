% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{w2ner,
  author       = {Jingye Li and
                  Hao Fei and
                  Jiang Liu and
                  Shengqiong Wu and
                  Meishan Zhang and
                  Chong Teng and
                  Donghong Ji and
                  Fei Li},
  title        = {Unified Named Entity Recognition as Word-Word Relation Classification},
  booktitle    = {Thirty-Sixth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2022, Thirty-Fourth Conference on Innovative Applications of Artificial
                  Intelligence, {IAAI} 2022, The Twelveth Symposium on Educational Advances
                  in Artificial Intelligence, {EAAI} 2022 Virtual Event, February 22
                  - March 1, 2022},
  pages        = {10965--10973},
  publisher    = {{AAAI} Press},
  year         = {2022},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/21344},
  timestamp    = {Tue, 12 Jul 2022 14:14:21 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/Li00WZTJL22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{instructuie,
	title = {{InstructUIE}: {Multi}-task {Instruction} {Tuning} for {Unified} {Information} {Extraction}},
	shorttitle = {{InstructUIE}},
	url = {http://arxiv.org/abs/2304.08085},
	doi = {10.48550/arXiv.2304.08085},
	abstract = {Large language models have unlocked strong multi-task capabilities from reading instructive prompts. However, recent studies have shown that existing large models still have difficulty with information extraction tasks. For example, gpt-3.5-turbo achieved an F1 score of 18.22 on the Ontonotes dataset, which is significantly lower than the state-of-the-art performance. In this paper, we propose InstructUIE, a unified information extraction framework based on instruction tuning, which can uniformly model various information extraction tasks and capture the inter-task dependency. To validate the proposed method, we introduce IE INSTRUCTIONS, a benchmark of 32 diverse information extraction datasets in a unified text-to-text format with expert-written instructions. Experimental results demonstrate that our method achieves comparable performance to Bert in supervised settings and significantly outperforms the state-of-the-art and gpt3.5 in zero-shot settings.},
	urldate = {2023-04-23},
	publisher = {arXiv},
	author = {Wang, Xiao and Zhou, Weikang and Zu, Can and Xia, Han and Chen, Tianze and Zhang, Yuansen and Zheng, Rui and Ye, Junjie and Zhang, Qi and Gui, Tao and Kang, Jihua and Yang, Jingsheng and Li, Siyuan and Du, Chunsai},
	month = apr,
	year = {2023},
	note = {arXiv:2304.08085 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:E\:\\Books and Papers\\Zotero\\storage\\4KS9RWIJ\\2304.html:text/html;Wang et al_2023_InstructUIE.pdf:E\:\\Books and Papers\\Zotero\\storage\\KEDABGNI\\Wang et al_2023_InstructUIE.pdf:application/pdf},
}

@misc{rexuie,
	title = {{RexUIE}: {A} {Recursive} {Method} with {Explicit} {Schema} {Instructor} for {Universal} {Information} {Extraction}},
	shorttitle = {{RexUIE}},
	url = {http://arxiv.org/abs/2304.14770},
	abstract = {Universal Information Extraction (UIE) is an area of interest due to the challenges posed by varying targets, heterogeneous structures, and demand-specific schemas. However, previous works have only achieved limited success by unifying a few tasks, such as Named Entity Recognition (NER) and Relation Extraction (RE), which fall short of being authentic UIE models particularly when extracting other general schemas such as quadruples and quintuples. Additionally, these models used an implicit structural schema instructor, which could lead to incorrect links between types, hindering the model's generalization and performance in low-resource scenarios. In this paper, we redefine the authentic UIE with a formal formulation that encompasses almost all extraction schemas. To the best of our knowledge, we are the first to introduce UIE for any kind of schemas. In addition, we propose RexUIE, which is a Recursive Method with Explicit Schema Instructor for UIE. To avoid interference between different types, we reset the position ids and attention mask matrices. RexUIE shows strong performance under both full-shot and few-shot settings and achieves State-of-the-Art results on the tasks of extracting complex schemas.},
	urldate = {2023-05-03},
	publisher = {arXiv},
	author = {Liu, Chengyuan and Zhao, Fubang and Kang, Yangyang and Zhang, Jingyuan and Zhou, Xiang and Sun, Changlong and Wu, Fei and Kuang, Kun},
	month = apr,
	year = {2023},
	note = {arXiv:2304.14770 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:E\:\\Books and Papers\\Zotero\\storage\\HBNVHK88\\2304.html:text/html;Liu et al_2023_RexUIE.pdf:E\:\\Books and Papers\\Zotero\\storage\\LE4PERJ4\\Liu et al_2023_RexUIE.pdf:application/pdf},
}

@misc{uniex,
	title = {{UniEX}: {An} {Effective} and {Efficient} {Framework} for {Unified} {Information} {Extraction} via a {Span}-extractive {Perspective}},
	shorttitle = {{UniEX}},
	url = {http://arxiv.org/abs/2305.10306},
	doi = {10.48550/arXiv.2305.10306},
	abstract = {We propose a new paradigm for universal information extraction (IE) that is compatible with any schema format and applicable to a list of IE tasks, such as named entity recognition, relation extraction, event extraction and sentiment analysis. Our approach converts the text-based IE tasks as the token-pair problem, which uniformly disassembles all extraction targets into joint span detection, classification and association problems with a unified extractive framework, namely UniEX. UniEX can synchronously encode schema-based prompt and textual information, and collaboratively learn the generalized knowledge from pre-defined information using the auto-encoder language models. We develop a traffine attention mechanism to integrate heterogeneous factors including tasks, labels and inside tokens, and obtain the extraction target via a scoring matrix. Experiment results show that UniEX can outperform generative universal IE models in terms of performance and inference-speed on \$14\$ benchmarks IE datasets with the supervised setting. The state-of-the-art performance in low-resource scenarios also verifies the transferability and effectiveness of UniEX.},
	urldate = {2023-05-18},
	publisher = {arXiv},
	author = {Lu, Junyu and Yang, Ping and Gan, Ruyi and Wang, Junjie and Zhang, Yuxiang and Zhang, Jiaxing and Zhang, Pingjian},
	month = may,
	year = {2023},
	note = {arXiv:2305.10306 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:E\:\\Books and Papers\\Zotero\\storage\\PIJ763G8\\2305.html:text/html;Lu et al_2023_UniEX.pdf:E\:\\Books and Papers\\Zotero\\storage\\ESVNJ6YW\\Lu et al_2023_UniEX.pdf:application/pdf},
}

@inproceedings{dygiepp,
    title = "Entity, Relation, and Event Extraction with Contextualized Span Representations",
    author = "Wadden, David  and
      Wennberg, Ulme  and
      Luan, Yi  and
      Hajishirzi, Hannaneh",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1585",
    doi = "10.18653/v1/D19-1585",
    pages = "5784--5789",
    abstract = "We examine the capabilities of a unified, multi-task framework for three information extraction tasks: named entity recognition, relation extraction, and event extraction. Our framework (called DyGIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local (within-sentence) and global (cross-sentence) context. Our framework achieves state-of-the-art results across all tasks, on four datasets from a variety of domains. We perform experiments comparing different techniques to construct span representations. Contextualized embeddings like BERT perform well at capturing relationships among entities in the same or adjacent sentences, while dynamic span graph updates model long-range cross-sentence relationships. For instance, propagating span representations via predicted coreference links can enable the model to disambiguate challenging entity mentions. Our code is publicly available at https://github.com/dwadden/dygiepp and can be easily adapted for new tasks or datasets.",
}

@inproceedings{mac-discontinuous-ner,
    title = "Discontinuous Named Entity Recognition as Maximal Clique Discovery",
    author = "Wang, Yucheng  and
      Yu, Bowen  and
      Zhu, Hongsong  and
      Liu, Tingwen  and
      Yu, Nan  and
      Sun, Limin",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.63",
    doi = "10.18653/v1/2021.acl-long.63",
    pages = "764--774",
    abstract = "Named entity recognition (NER) remains challenging when entity mentions can be discontinuous. Existing methods break the recognition process into several sequential steps. In training, they predict conditioned on the golden intermediate results, while at inference relying on the model output of the previous steps, which introduces exposure bias. To solve this problem, we first construct a segment graph for each sentence, in which each node denotes a segment (a continuous entity on its own, or a part of discontinuous entities), and an edge links two nodes that belong to the same entity. The nodes and edges can be generated respectively in one stage with a grid tagging scheme and learned jointly using a novel architecture named Mac. Then discontinuous NER can be reformulated as a non-parametric process of discovering maximal cliques in the graph and concatenating the spans in each clique. Experiments on three benchmarks show that our method outperforms the state-of-the-art (SOTA) results, with up to 3.5 percentage points improvement on F1, and achieves 5x speedup over the SOTA model.",
}

@inproceedings{ptpcg,
  title     = {Efficient Document-level Event Extraction via Pseudo-Trigger-aware Pruned Complete Graph},
  author    = {Zhu, Tong and Qu, Xiaoye and Chen, Wenliang and Wang, Zhefeng and Huai, Baoxing and Yuan, Nicholas and Zhang, Min},
  booktitle = {Proceedings of the Thirty-First International Joint Conference on
               Artificial Intelligence, {IJCAI-22}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Lud De Raedt},
  pages     = {4552--4558},
  year      = {2022},
  month     = {7},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2022/632},
  url       = {https://doi.org/10.24963/ijcai.2022/632},
}

@inproceedings{bart-ner,
    title = "A Unified Generative Framework for Various {NER} Subtasks",
    author = "Yan, Hang  and
      Gui, Tao  and
      Dai, Junqi  and
      Guo, Qipeng  and
      Zhang, Zheng  and
      Qiu, Xipeng",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.451",
    doi = "10.18653/v1/2021.acl-long.451",
    pages = "5808--5822",
    abstract = "Named Entity Recognition (NER) is the task of identifying spans that represent entities in sentences. Whether the entity spans are nested or discontinuous, the NER task can be categorized into the flat NER, nested NER, and discontinuous NER subtasks. These subtasks have been mainly solved by the token-level sequence labelling or span-level classification. However, these solutions can hardly tackle the three kinds of NER subtasks concurrently. To that end, we propose to formulate the NER subtasks as an entity span sequence generation task, which can be solved by a unified sequence-to-sequence (Seq2Seq) framework. Based on our unified framework, we can leverage the pre-trained Seq2Seq model to solve all three kinds of NER subtasks without the special design of the tagging schema or ways to enumerate spans. We exploit three types of entity representations to linearize entities into a sequence. Our proposed framework is easy-to-implement and achieves state-of-the-art (SoTA) or near SoTA performance on eight English NER datasets, including two flat NER datasets, three nested NER datasets, and three discontinuous NER datasets.",
}

@inproceedings{bart-absa,
    title = "A Unified Generative Framework for Aspect-based Sentiment Analysis",
    author = "Yan, Hang  and
      Dai, Junqi  and
      Ji, Tuo  and
      Qiu, Xipeng  and
      Zhang, Zheng",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.188",
    doi = "10.18653/v1/2021.acl-long.188",
    pages = "2416--2429",
    abstract = "Aspect-based Sentiment Analysis (ABSA) aims to identify the aspect terms, their corresponding sentiment polarities, and the opinion terms. There exist seven subtasks in ABSA. Most studies only focus on the subsets of these subtasks, which leads to various complicated ABSA models while hard to solve these subtasks in a unified framework. In this paper, we redefine every subtask target as a sequence mixed by pointer indexes and sentiment class indexes, which converts all ABSA subtasks into a unified generative formulation. Based on the unified formulation, we exploit the pre-training sequence-to-sequence model BART to solve all ABSA subtasks in an end-to-end framework. Extensive experiments on four ABSA datasets for seven subtasks demonstrate that our framework achieves substantial performance gain and provides a real unified end-to-end solution for the whole ABSA subtasks, which could benefit multiple tasks.",
}

@inproceedings{oneie,
    title = "A Joint Neural Model for Information Extraction with Global Features",
    author = "Lin, Ying  and
      Ji, Heng  and
      Huang, Fei  and
      Wu, Lingfei",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.713",
    doi = "10.18653/v1/2020.acl-main.713",
    pages = "7999--8009",
    abstract = "Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. For example, a victim of a die event is likely to be a victim of an attack event in the same sentence. In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, OneIE, that aims to extract the globally optimal IE result as a graph from an input sentence. OneIE performs end-to-end IE in four stages: (1) Encoding a given sentence as contextualized word representations; (2) Identifying entity mentions and event triggers as nodes; (3) Computing label scores for all nodes and their pairwise links using local classifiers; (4) Searching for the globally optimal graph with a beam decoder. At the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions. Experiments show that adding global features improves the performance of our model and achieves new state of-the-art on all subtasks. In addition, as OneIE does not use any language-specific feature, we prove it can be easily applied to new languages or trained in a multilingual manner.",
}

@inproceedings{genie,
    title = "{G}en{IE}: Generative Information Extraction",
    author = "Josifoski, Martin  and
      De Cao, Nicola  and
      Peyrard, Maxime  and
      Petroni, Fabio  and
      West, Robert",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.342",
    doi = "10.18653/v1/2022.naacl-main.342",
    pages = "4626--4643",
    abstract = "Structured and grounded representation of text is typically formalized by closed information extraction, the problem of extracting an exhaustive set of (subject, relation, object) triplets that are consistent with a predefined set of entities and relations from a knowledge base schema. Most existing works are pipelines prone to error accumulation, and all approaches are only applicable to unrealistically small numbers of entities and relations. We introduce GenIE (generative information extraction), the first end-to-end autoregressive formulation of closed information extraction. GenIE naturally exploits the language knowledge from the pre-trained transformer by autoregressively generating relations and entities in textual form. Thanks to a new bi-level constrained generation strategy, only triplets consistent with the predefined knowledge base schema are produced. Our experiments show that GenIE is state-of-the-art on closed information extraction, generalizes from fewer training data points than baselines, and scales to a previously unmanageable number of entities and relations. With this work, closed information extraction becomes practical in realistic scenarios, providing new opportunities for downstream tasks. Finally, this work paves the way towards a unified end-to-end approach to the core tasks of information extraction.",
}

@inproceedings{tanl,
  author       = {Giovanni Paolini and
                  Ben Athiwaratkun and
                  Jason Krone and
                  Jie Ma and
                  Alessandro Achille and
                  Rishita Anubhai and
                  C{\'{\i}}cero Nogueira dos Santos and
                  Bing Xiang and
                  Stefano Soatto},
  title        = {Structured Prediction as Translation between Augmented Natural Languages},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=US-TP-xnXI},
  timestamp    = {Wed, 28 Jul 2021 14:18:31 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/PaoliniAKMAASXS21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{deepstruct,
    title = "{D}eep{S}truct: Pretraining of Language Models for Structure Prediction",
    author = "Wang, Chenguang  and
      Liu, Xiao  and
      Chen, Zui  and
      Hong, Haoyun  and
      Tang, Jie  and
      Song, Dawn",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.67",
    doi = "10.18653/v1/2022.findings-acl.67",
    pages = "803--823",
    abstract = "We introduce a method for improving the structural understanding abilities of language models. Unlike previous approaches that finetune the models with task-specific augmentation, we pretrain language models to generate structures from the text on a collection of task-agnostic corpora. Our structure pretraining enables zero-shot transfer of the learned knowledge that models have about the structure tasks. We study the performance of this approach on 28 datasets, spanning 10 structure prediction tasks including open information extraction, joint entity and relation extraction, named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, factual probe, intent detection, and dialogue state tracking. We further enhance the pretraining with the task-specific training sets. We show that a 10B parameter language model transfers non-trivially to most tasks and obtains state-of-the-art performance on 21 of 28 datasets that we evaluate. Our code and datasets will be made publicly available.",
}

@misc{devil-ee,
	title = {The {Devil} is in the {Details}: {On} the {Pitfalls} of {Event} {Extraction} {Evaluation}},
	shorttitle = {The {Devil} is in the {Details}},
	url = {http://arxiv.org/abs/2306.06918},
	abstract = {Event extraction (EE) is a crucial task aiming at extracting events from texts, which includes two subtasks: event detection (ED) and event argument extraction (EAE). In this paper, we check the reliability of EE evaluations and identify three major pitfalls: (1) The data preprocessing discrepancy makes the evaluation results on the same dataset not directly comparable, but the data preprocessing details are not widely noted and specified in papers. (2) The output space discrepancy of different model paradigms makes different-paradigm EE models lack grounds for comparison and also leads to unclear mapping issues between predictions and annotations. (3) The absence of pipeline evaluation of many EAE-only works makes them hard to be directly compared with EE works and may not well reflect the model performance in real-world pipeline scenarios. We demonstrate the significant influence of these pitfalls through comprehensive meta-analyses of recent papers and empirical experiments. To avoid these pitfalls, we suggest a series of remedies, including specifying data preprocessing, standardizing outputs, and providing pipeline evaluation results. To help implement these remedies, we develop a consistent evaluation framework OMNIEVENT, which can be obtained from https://github.com/THU-KEG/OmniEvent.},
	urldate = {2023-06-13},
	publisher = {arXiv},
	author = {Hao, Peng and Xiaozhi, Wang and Feng, Yao and Kaisheng, Zeng and Lei, Hou and Juanzi, Li and Zhiyuan, Liu and Weixing, Shen},
	month = jun,
	year = {2023},
	note = {arXiv:2306.06918 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:E\:\\Books and Papers\\Zotero\\storage\\NKIPM3T5\\2306.html:text/html;Full Text PDF:E\:\\Books and Papers\\Zotero\\storage\\F6J8VXS5\\Hao 等 - 2023 - The Devil is in the Details On the Pitfalls of Ev.pdf:application/pdf},
}

@inproceedings{lasuie,
  author       = {Hao Fei and
                  Shengqiong Wu and
                  Jingye Li and
                  Bobo Li and
                  Fei Li and
                  Libo Qin and
                  Meishan Zhang and
                  Min Zhang and
                  Tat{-}Seng Chua},
  title        = {LasUIE: Unifying Information Extraction with Latent Adaptive Structure-aware
                  Generative Language Model},
  booktitle    = {NeurIPS},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/63943ee9fe347f3d95892cf87d9a42e6-Abstract-Conference.html},
  timestamp    = {Thu, 11 May 2023 17:08:21 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/0001WLLLQZZC22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{uie,
    title = "Unified Structure Generation for Universal Information Extraction",
    author = "Lu, Yaojie  and
      Liu, Qing  and
      Dai, Dai  and
      Xiao, Xinyan  and
      Lin, Hongyu  and
      Han, Xianpei  and
      Sun, Le  and
      Wu, Hua",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.395",
    doi = "10.18653/v1/2022.acl-long.395",
    pages = "5755--5772",
    abstract = "Information extraction suffers from its varying targets, heterogeneous structures, and demand-specific schemas. In this paper, we propose a unified text-to-structure generation framework, namely UIE, which can universally model different IE tasks, adaptively generate targeted structures, and collaboratively learn general IE abilities from different knowledge sources. Specifically, UIE uniformly encodes different extraction structures via a structured extraction language, adaptively generates target extractions via a schema-based prompt mechanism {--} structural schema instructor, and captures the common IE abilities via a large-scale pretrained text-to-structure model. Experiments show that UIE achieved the state-of-the-art performance on 4 IE tasks, 13 datasets, and on all supervised, low-resource, and few-shot settings for a wide range of entity, relation, event and sentiment extraction tasks and their unification. These results verified the effectiveness, universality, and transferability of UIE.",
}

# usm is published at AAAI'23, while the official proceedings didn't come out yet, use arxiv version
@article{usm,
  author       = {Jie Lou and
                  Yaojie Lu and
                  Dai Dai and
                  Wei Jia and
                  Hongyu Lin and
                  Xianpei Han and
                  Le Sun and
                  Hua Wu},
  title        = {Universal Information Extraction as Unified Semantic Matching},
  journal      = {CoRR},
  volume       = {abs/2301.03282},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2301.03282},
  doi          = {10.48550/arXiv.2301.03282},
  eprinttype    = {arXiv},
  eprint       = {2301.03282},
  timestamp    = {Tue, 10 Jan 2023 15:10:12 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2301-03282.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{unimc,
    title = "Zero-Shot Learners for Natural Language Understanding via a Unified Multiple Choice Perspective",
    author = "Yang, Ping  and
      Wang, Junjie  and
      Gan, Ruyi  and
      Zhu, Xinyu  and
      Zhang, Lin  and
      Wu, Ziwei  and
      Gao, Xinyu  and
      Zhang, Jiaxing  and
      Sakai, Tetsuya",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.474",
    pages = "7042--7055",
    abstract = "We propose a new paradigm for zero-shot learners that is format agnostic, i.e., it is compatible with any format and applicable to a list of language tasks, such as text classification, commonsense reasoning, coreference resolution, and sentiment analysis. Zero-shot learning aims to train a model on a given task such that it can address new learning tasks without any additional training. Our approach converts zero-shot learning into multiple-choice tasks, avoiding problems in commonly used large-scale generative models such as FLAN. It not only adds generalization ability to models but also significantly reduces the number of parameters. Our method shares the merits of efficient training and deployment. Our approach shows state-of-the-art performance on several benchmarks and produces satisfactory results on tasks such as natural language inference and text classification. Our model achieves this success with only 235M parameters, which is substantially smaller than state-of-the-art models with billions of parameters. The code and pre-trained models are available at https://github.com/IDEA-CCNL/Fengshenbang-LM/tree/main/fengshen/examples/unimc .",
}

@inproceedings{bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}

@article{t5,
  author       = {Colin Raffel and
                  Noam Shazeer and
                  Adam Roberts and
                  Katherine Lee and
                  Sharan Narang and
                  Michael Matena and
                  Yanqi Zhou and
                  Wei Li and
                  Peter J. Liu},
  title        = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
                  Transformer},
  journal      = {J. Mach. Learn. Res.},
  volume       = {21},
  pages        = {140:1--140:67},
  year         = {2020},
  url          = {http://jmlr.org/papers/v21/20-074.html},
  timestamp    = {Fri, 05 Feb 2021 15:43:41 +0100},
  biburl       = {https://dblp.org/rec/journals/jmlr/RaffelSRLNMZLL20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{glm,
    title = "{GLM}: General Language Model Pretraining with Autoregressive Blank Infilling",
    author = "Du, Zhengxiao  and
      Qian, Yujie  and
      Liu, Xiao  and
      Ding, Ming  and
      Qiu, Jiezhong  and
      Yang, Zhilin  and
      Tang, Jie",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.26",
    doi = "10.18653/v1/2022.acl-long.26",
    pages = "320--335",
    abstract = "There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25{\mbox{$\times$}} parameters of BERT Large , demonstrating its generalizability to different downstream tasks.",
}

@article{flan-t5,
  author       = {Hyung Won Chung and
                  Le Hou and
                  Shayne Longpre and
                  Barret Zoph and
                  Yi Tay and
                  William Fedus and
                  Eric Li and
                  Xuezhi Wang and
                  Mostafa Dehghani and
                  Siddhartha Brahma and
                  Albert Webson and
                  Shixiang Shane Gu and
                  Zhuyun Dai and
                  Mirac Suzgun and
                  Xinyun Chen and
                  Aakanksha Chowdhery and
                  Sharan Narang and
                  Gaurav Mishra and
                  Adams Yu and
                  Vincent Y. Zhao and
                  Yanping Huang and
                  Andrew M. Dai and
                  Hongkun Yu and
                  Slav Petrov and
                  Ed H. Chi and
                  Jeff Dean and
                  Jacob Devlin and
                  Adam Roberts and
                  Denny Zhou and
                  Quoc V. Le and
                  Jason Wei},
  title        = {Scaling Instruction-Finetuned Language Models},
  journal      = {CoRR},
  volume       = {abs/2210.11416},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2210.11416},
  doi          = {10.48550/arXiv.2210.11416},
  eprinttype    = {arXiv},
  eprint       = {2210.11416},
  timestamp    = {Wed, 26 Oct 2022 08:16:51 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2210-11416.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ner-as-dp,
    title = "Named Entity Recognition as Dependency Parsing",
    author = "Yu, Juntao  and
      Bohnet, Bernd  and
      Poesio, Massimo",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.577",
    doi = "10.18653/v1/2020.acl-main.577",
    pages = "6470--6476",
    abstract = "Named Entity Recognition (NER) is a fundamental task in Natural Language Processing, concerned with identifying spans of text expressing references to entities. NER research is often focused on flat entities only (flat NER), ignoring the fact that entity references can be nested, as in [Bank of [China]] (Finkel and Manning, 2009). In this paper, we use ideas from graph-based dependency parsing to provide our model a global view on the input via a biaffine model (Dozat and Manning, 2017). The biaffine model scores pairs of start and end tokens in a sentence which we use to explore all spans, so that the model is able to predict named entities accurately. We show that the model works well for both nested and flat NER through evaluation on 8 corpora and achieving SoTA performance on all of them, with accuracy gains of up to 2.2 percentage points.",
}


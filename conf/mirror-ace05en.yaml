# task
task_type: SchemaGuidedInstructBertTask
task_name: InstructBert_TagSpan_DebertaV3Base_ACE05EN_NerRelEvent
comment: '~~content as label, (start, end + 1) span'

# data preprocessing
max_seq_len: 512
debug_mode: false
label_span: tag  # tag `[LM]` or content `person`
mode: span  # w2 (1,2,3) or span (1,3)

# filepaths
plm_dir: microsoft/deberta-v3-base
# plm_dir: bert-base-cased
data_dir: resources/Mirror/Tasks/EE/ACE05-EN
output_dir: outputs
task_dir: ${output_dir}/${task_name}
# train_filepath: ${data_dir}/ACE2005_plus_train.jsonl
# dev_filepath: ${data_dir}/ACE2005_plus_dev.jsonl
# test_filepath: ${data_dir}/ACE2005_plus_test.jsonl
# train_filepath: ${data_dir}/ACE2005_oneie_RE_train.jsonl
# dev_filepath: ${data_dir}/ACE2005_oneie_RE_dev.jsonl
# test_filepath: ${data_dir}/ACE2005_oneie_RE_test.jsonl
train_filepath: ${data_dir}/ACE2005_oneie_train.jsonl
dev_filepath: ${data_dir}/ACE2005_oneie_dev.jsonl
test_filepath: ${data_dir}/ACE2005_oneie_test.jsonl
dump_cache_dir: ${task_dir}/cache
regenerate_cache: true

# training
random_seed: 1227
eval_on_data: [dev, test]
select_best_on_data: dev
select_best_by_key: metric
best_metric_field: general_spans.micro.f1
final_eval_on_test: true

warmup_proportion: 0.1
num_epochs: 10
epoch_patience: 5
train_batch_size: 16
eval_batch_size: 16
learning_rate: !!float 2e-5
other_learning_rate: !!float 1e-4
max_grad_norm: 1.0
weight_decay: 0.1

# model
dropout: 0.3
use_rope: true
biaffine_size: 512
